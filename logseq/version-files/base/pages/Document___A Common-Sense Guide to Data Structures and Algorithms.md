title:: Document/A Common-Sense Guide to Data Structures and Algorithms
author:: [[@Jay Wengrow]]
abbreviated-title:: A Common-Sense Guide to Data Structures and Algorithms 
cover:: ![Image](https://m.media-amazon.com/images/I/813nURWkkbL._SY160.jpg)
full-title:: A Common-Sense Guide to Data Structures and Algorithms
start:: [[Oct 24th, 2023]]
end:: Unfinished
status:: Reading
document_type::

- Highlights
	- ```A computer can read from an array in just one step.``` ([Location 396](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=396))
	- ```A computer can jump to any memory address in one step.``` ([Location 411](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=411))
	- ```Whenever a computer allocates an array, it also makes note at which memory address the array begins.``` ([Location 413](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=413))
	- ```a computer can also find the value at any index by performing simple addition.``` ([Location 416](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=416))
	- ```Searching, on the other hand, means providing the computer a value and asking it to return the index of that value’s location.``` ([Location 432](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=432))
	- ```Reading from an index is fast, since a computer can jump immediately to any index and discover the value contained there. Searching, though, is tedious, since the computer has no way to jump to a particular value.``` ([Location 434](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=434))
	- ```a computer has immediate access to all of its memory addresses, but it has no idea offhand what values are contained at each memory address.``` ([Location 436](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=436))
	- ```this basic search operation—in which the computer checks each cell one at a time—is known as linear search.``` ([Location 451](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=451))
	- ```when allocating an array, the computer always keeps track of the array’s size.``` ([Location 464](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=464))
	- ```Because the computer initially allocated only five cells in memory for the array, and now we’re adding a sixth element, the computer may have to allocate additional cells toward this array. In many programming languages, this is done under the hood automatically, but each language handles this differently, so I won’t get into the details of it.``` ([Location 470](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=470))
	- ```The worst-case scenario for insertion into an array—that is, the scenario in which insertion takes the most steps—is when we insert data at the beginning of the array. This is because when inserting at the beginning of the array, we have to move all the other values one cell to the right.``` ([Location 488](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=488))
	- ```We can say that insertion in a worst-case scenario can take N + 1 steps for an array containing N elements.``` ([Location 490](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=490))
	- ```Like insertion, the worst-case scenario of deleting an element is deleting the very first element of the array. This is because index 0 would become empty, and we’d have to shift all the remaining elements to the left to fill the gap.``` ([Location 503](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=503))
	- ```We can say then, that for an array containing N elements, the maximum number of steps that deletion would take is N steps.``` ([Location 507](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=507))
	- ```I’ll talk about an array-based set. This set is just like an array—it is a simple list of values. The only difference between this set and a classic array is that the set never allows duplicate values to be inserted into it.``` ([Location 514](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=514))
	- ```Sets are useful when you need to ensure that you don’t have duplicate data.``` ([Location 518](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=518))
	- ```In any case, an array-based set is an array with one additional constraint of barring duplicates.``` ([Location 524](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=524))
	- ```Reading from a set is exactly the same as reading from an array—it takes just one step for the computer to look up what’s contained within a particular index.``` ([Location 527](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=527))
	- ```Searching a set also turns out to be no different than searching an array—it takes up to N steps to search for a value within a set.``` ([Location 529](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=529))
	- ```deletion is also identical between a set and an array—it takes up to N steps to delete a value and move data to the left to close the gap.``` ([Location 530](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=530))
	- ```the computer will first need to search the set to see whether the value we want to insert is already there.``` ([Location 535](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=535))
	- ```Only if the set does not yet contain our new value will the computer allow the insertion to take place.``` ([Location 536](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=536))
	- ```every insertion into a set first requires a search.``` ([Location 537](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=537))
	- ```Inserting a value at the end of a set is the best-case scenario,``` ([Location 550](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=550))
	- ```insertion into the end of a set will take up to N + 1 steps for N elements.``` ([Location 552](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=552))
	- ```In the worst-case scenario, where we’re inserting a value at the beginning of a set, the computer needs to search N cells to ensure that the set doesn’t already contain that value, another N steps to shift all the data to the right, and another final step to insert the new value. That’s a total of 2N + 1 steps.``` ([Location 554](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=554))
	- ```Contrast this to insertion into the beginning of a regular array, which only takes N + 1 steps.``` ([Location 556](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=556))
	- ```another major factor can affect the efficiency of our code: the proper selection of which algorithm to use.``` ([Location 581](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=581))
	- ```An algorithm is simply a set of instructions for completing a specific task.``` ([Location 583](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=583))
	- ```When applied to computing, an algorithm refers to the set of instructions given to a computer to achieve a particular task.``` ([Location 587](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=587))
	- ```The ordered array is almost identical to the “classic” array we saw in the previous chapter. The only difference is that ordered arrays require that the values are always kept—you guessed it—in order. That is, every time a value is added, it gets placed in the proper cell so that the values in the array remain sorted.``` ([Location 595](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=595))
	- ```The computer cannot simply drop the 75 into the right slot in a single step because it first has to find the right place to insert the 75, and then shift the other values to make room for it.``` ([Location 603](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=603))
	- ```the ordered array has a secret superpower when it comes to searching.``` ([Location 625](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=625))
	- ```Let’s say we’re searching for a 22 within an ordered array of [3, 17, 75, 80, 202]. We can stop the search as soon as we reach the 75, since it’s impossible for the 22 to be anywhere to the right of it.``` ([Location 633](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=633))
	- ```Note that binary search is only possible within an ordered array. With a classic array, values can be in any order, and we’d never know whether to look to the left or right of any given value. This is one of the advantages of ordered arrays: we have the option of binary search.``` ([Location 704](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=704))
	- ```With ordered arrays of a small size, the algorithm of binary search doesn’t have much of an advantage over linear search.``` ([Location 816](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=816))
	- ```This pattern is unusually efficient: each time we double the data, the binary search algorithm adds just one more step.``` ([Location 826](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=826))
	- ```With linear search, then, there are as many steps as there are items.``` ([Location 829](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=829))
	- ```With binary search, on the other hand, you’ll see that as the data increases, the algorithm’s steps only increase marginally.``` ([Location 839](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=839))
	- ```you have somewhat slower insertion, but much faster search.``` ([Location 843](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=843))
	- ```double the amount of data, we add only one step. After all, this doubling of data gets totally eliminated with the first inspection!``` ([Location 849](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=849))
	- ```It’s also important to realize that there usually isn’t a single data structure or algorithm that is perfect for every situation.``` ([Location 855](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=855))
	- ```the way to analyze competing algorithms is to count the number of steps each one takes.``` ([Location 858](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=858))
	- ```The more effective way, then, to quantify the efficiency of linear search is to say that linear search takes N steps for N elements in the array.``` ([Location 871](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=871))
	- ```Known as Big O Notation, this formalized expression of these concepts allows us to easily categorize the efficiency of a given algorithm and convey it to others.``` ([Location 875](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=875))
	- ```As we’ve previously phrased it: for N elements in the array, linear search can take up to N steps. The appropriate way to express this in Big O Notation is: O(N)``` ([Location 883](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=883))
	- ```Here’s what the notation means. It expresses the answer to what we’ll call the “key question.” The key question is: if there are N data elements, how many steps will the algorithm take?``` ([Location 885](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=885))
	- ```O(N) says that the answer to the key question is that the algorithm will take N steps.``` ([Location 889](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=889))
	- ```Because the answer to this question is that linear search will take N steps, we express this as O(N). For the record, an algorithm that is O(N) is also known as having linear time.``` ([Location 891](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=891))
	- ```if there are N data elements, how many steps will reading from an array take? The answer is that reading takes just one step. So, we express this as O(1), which I pronounce “Oh of 1.”``` ([Location 895](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=895))
	- ```O(1) is considered the “fastest” kind of algorithm.``` ([Location 899](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=899))
	- ```Even as the data increases, an O(1) algorithm doesn’t take any additional steps.``` ([Location 900](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=900))
	- ```In fact, an O(1) algorithm can also be referred to as having constant time.``` ([Location 901](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=901))
	- ```one way of describing Big O is that it describes the upper bound of the growth rate of a function, or that if a function g(x) grows no faster than a function f(x), then g is said to be a member of O(f).``` ([Location 905](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=905))
	- ```Now that we’ve encountered O(N) and O(1), we begin to see that Big O Notation does more than simply describe the number of steps an algorithm takes,``` ([Location 912](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=912))
	- ```it’s an answer to that key question on your forehead: if there are N data elements, how many steps will the algorithm take?``` ([Location 914](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=914))
	- ```how will an algorithm’s performance change as the data increases?``` ([Location 920](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=920))
	- ```This is the soul of Big O. Big O doesn’t want to simply tell you how many steps an algorithm takes. It wants to tell you the story of how the number of steps increase as the data changes.``` ([Location 921](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=921))
	- ```It tells you about the proportional relationship between the data and the algorithm’s efficiency.``` ([Location 927](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=927))
	- ```It describes exactly how the number of steps increase as the data increases.``` ([Location 928](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=928))
	- ```The same is true even for an O(1) algorithm that always takes one million steps. As the data increases, there will inevitably reach a point at which O(N) becomes less efficient than the O(1) algorithm, and will remain so up toward an infinite amount of data.``` ([Location 939](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=939))
	- ```If we were to describe the efficiency of linear search in its totality, we’d say that linear search is``` ([Location 944](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=944))
	- ```O(1) in a best-case scenario, and O(N) in a worst-case scenario.``` ([Location 945](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=945))
	- ```While Big O effectively describes both the best- and worst-case scenarios of a given algorithm, Big O Notation generally refers to the worst-case scenario unless specified otherwise.``` ([Location 946](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=946))
	- ```knowing exactly how inefficient an algorithm can get in a worst-case scenario prepares us for the worst and may have a strong impact on our choices.``` ([Location 949](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=949))
	- ```In Big O terms, we describe binary search as having a time complexity of: O(log N)``` ([Location 955](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=955))
	- ```This type of algorithm is also known as having a time complexity of log time.``` ([Location 956](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=956))
	- ```O(log N) is the Big O way of describing an algorithm that increases one step each time the data is doubled.``` ([Location 957](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=957))
	- ```To understand why this algorithm is called “O(log N),” you need to first understand what logarithms are.``` ([Location 962](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=962))
	- ```Logarithms are the inverse of exponents.``` ([Location 967](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=967))
	- ```Now, log2 8 is the converse. It means: how many times do you have to multiply 2 by itself to get a result of 8?``` ([Location 968](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=968))
	- ```Another way of explaining log2 8 is: if we kept dividing 8 by 2 until we ended up with 1, how many 2s would we have in our equation?``` ([Location 973](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=973))
	- ```In computer science, whenever we say O(log N), it’s actually shorthand for saying O(log2 N). We just omit that small 2 for convenience.``` ([Location 978](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=978))
	- ```Recall that Big O Notation resolves the key question: if there are N data elements, how many steps will the algorithm take?``` ([Location 980](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=980))
	- ```O(log N) means that for N data elements, the algorithm would take log2 N steps``` ([Location 980](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=980))
	- ```O(log N) means the algorithm takes as many steps as it takes to keep halving the data elements until we remain with 1.``` ([Location 984](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=984))
	- ```With Big O Notation, we have a consistent system that allows us to compare any two algorithms.``` ([Location 1037](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1037))
	- ```With Big O, you also have the opportunity to compare your algorithm to general algorithms out there in the world,``` ([Location 1120](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1120))
	- ```If you find that Big O labels your algorithm as a “slow” one, you can now take a step back and try to figure out if there’s a way to optimize it by trying to get it to fall under a faster category of Big O.``` ([Location 1122](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1122))
	- ```They all solve the following problem: Given an array of unsorted values, how can we sort them so that they end up in ascending order?``` ([Location 1129](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1129))
	- ```O(N2) is considered to be a relatively inefficient algorithm, since as the data increases, the steps increase dramatically.``` ([Location 1270](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1270))
	- ```One last note: O(N2) is also referred to as quadratic time.``` ([Location 1273](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1273))
	- ```Very often (but not always), when an algorithm nests one loop inside another, the algorithm is O(N2).``` ([Location 1339](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1339))
	- ```Whenever you encounter a slow algorithm, it’s worth spending some time to consider whether there are any faster alternatives.``` ([Location 1342](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1342))
	- ```While you and I can both see that the entire array is correctly sorted at this point, the computer does not know this yet, so it must begin a fourth pass-through.``` ([Location 1530](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1530))
	- ```Because all the cells besides the last one are correctly sorted, that must mean the last cell is also in the correct order,``` ([Location 1535](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1535))
	- ```As for swaps, though, we only need to make a maximum of one swap per pass-through. This is because in each pass-through, we make either one or zero swaps, depending on whether the lowest number of that pass-through is already in the correct position. Contrast this with Bubble Sort, where in a worst-case scenario—an array in descending order—we have to make a swap for each and every comparison.``` ([Location 1605](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1605))
	- ```it’s clear Selection Sort takes about half the number of steps Bubble Sort does, indicating that Selection Sort is twice as fast.``` ([Location 1614](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1614))
	- ```Big O Notation ignores constants.``` ([Location 1626](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1626))
	- ```An algorithm that takes N2 + 10 steps would be expressed as O(N2),``` ([Location 1629](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1629))
	- ```With an algorithm that takes 2N steps (meaning N * 2), we drop the regular number and call it O(N).``` ([Location 1630](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1630))
	- ```Even O(100N), which is 100 times slower than O(N), is also referred to as O(N).``` ([Location 1631](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1631))
	- ```Big O Notation only concerns itself with general categories of algorithm speeds.``` ([Location 1636](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1636))
	- ```In other words, if there are N elements in the array, a maximum of N - 1 comparisons are made in the final pass-through.``` ([Location 1908](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1908))
	- ```We can, therefore, formulate the total number of comparisons as: 1 + 2 + 3 + … + (N - 1) comparisons.``` ([Location 1909](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1909))
	- ```When examining this pattern, it emerges that for an array containing N elements, there are approximately N2 / 2 comparisons.``` ([Location 1912](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1912))
	- ```Big O Notation only takes into account the highest order of N when we have multiple orders added together.``` ([Location 1924](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1924))
	- ```As N increases, N4 becomes so much more significant than any other order of N that the smaller orders are considered trivial.``` ([Location 1931](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1931))
	- ```It emerges that in a worst-case scenario, Insertion Sort has the same time complexity as Bubble Sort and Selection Sort. They’re all O(N2).``` ([Location 1935](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1935))
	- ```Indeed, in a worst-case scenario, Selection Sort is faster than Insertion Sort. However, it is critical we also take into account the average-case scenario.``` ([Location 1941](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1941))
	- ```For the average scenario, we can say that in the aggregate, we probably compare and shift about half the data. Thus, if Insertion Sort takes N2 steps for the worst-case scenario, we’d say that it takes about N2 / 2 steps for the average scenario. (In terms of Big O, however, both scenarios are O(N2).)``` ([Location 1957](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1957))
	- ```We can now see that the performance of Insertion Sort varies greatly based on the scenario.``` ([Location 1965](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1965))
	- ```Contrast this with Selection Sort. Selection Sort takes N2 / 2 steps in all cases,``` ([Location 1969](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1969))
	- ```This is because Selection Sort doesn’t have any mechanism for ending a pass-through early at any point.``` ([Location 1970](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1970))
	- ```In an average case—where an array is randomly sorted—they perform similarly. If you have reason to assume you’ll be dealing with data that is mostly sorted, Insertion Sort will be a better choice. If you have reason to assume you’ll be dealing with data that is mostly sorted in reverse order, Selection Sort will``` ([Location 1976](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1976))
	- ```be faster.``` ([Location 1978](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=1978))
	- ```If the arrays are different sizes—say N and M—we’d say that the efficiency of this function is O(N * M).``` ([Location 2010](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2010))
	- ```At the end of the day, our inner loop only runs for as many numbers as there are in total.``` ([Location 2366](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2366))
	- ```we can say that N represents how many numbers there are.``` ([Location 2366](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2366))
	- ```You can also see at a glance that half of the squares are shaded. This demonstrates that the pattern of N + (N - 1) + (N - 2) + (N - 3)… + 1 is equivalent to N2 / 2.``` ([Location 2455](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2455))
	- ```We have no choice but to express the time complexity as O(N * M), where N is the size of one array, and M is the size of the other.``` ([Location 2488](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2488))
	- ```whenever we have two distinct datasets that have to interact with each other through multiplication, we have to identify both sources separately when we describe the efficiency in terms of Big O.``` ([Location 2489](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2489))
	- ```we do know that there’s a specific range in which O(N * M) lies. That is, if N and M are the same, it’s equivalent to O(N2). And if they’re not the same, and we arbitrarily assign the smaller number to be M, even if M is as low as 1, we end up with O(N). In a sense then, O(N * M) can be construed as a range between O(N) and O(N2).``` ([Location 2492](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2492))
	- ```If we look at this in terms of N, it emerges that if N is the length of each string, the number of combinations is 26N.``` ([Location 2543](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2543))
	- ```The truth is that even an algorithm that is a “mere” O(2N) is incredibly slow.``` ([Location 2544](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2544))
	- ```if this array were unordered, searching for the price of a given food would take O(N) steps since the computer would have to perform a linear search. If it’s an ordered array, the computer could do a binary search, which would take O(log N).``` ([Location 2749](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2749))
	- ```special data structure called a hash table, which can be used to look up data in just O(1) time.``` ([Location 2751](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2751))
	- ```Most programming languages include a data structure called a hash table, and it has an amazing superpower: fast reading.``` ([Location 2754](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2754))
	- ```A hash table is a list of paired values. The first item in each pair is called the key, and the second item is called the value.``` ([Location 2761](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2761))
	- ```In a hash table, the key and value have some significant association with one another.``` ([Location 2762](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2762))
	- ```Looking up a value in a hash table has an efficiency of O(1) on average, as it usually takes just one step.``` ([Location 2766](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2766))
	- ```This process of taking characters and converting them to numbers is known as hashing. And the code that is used to convert those letters into particular numbers is called a hash function.``` ([Location 2771](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2771))
	- ```The truth is that a hash function needs to meet only one criterion to be valid: a hash function must convert the same string to the same number every single time it’s applied.``` ([Location 2779](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2779))
	- ```a hash table stores its data in a bunch of cells in a row, similar to an array. Each cell has a corresponding number.``` ([Location 2794](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2794))
	- ```When we look up items from a hash table, we use a key to find its associated value.``` ([Location 2818](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2818))
	- ```Just as we hashed the key to insert the value``` ([Location 2828](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2828))
	- ```in the appropriate cell, we can hash the key again to find where we previously put that value.``` ([Location 2828](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2828))
	- ```The computer hashes the key, turns it into a number, and jumps to the index with that number to retrieve the value stored there.``` ([Location 2830](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2830))
	- ```It’s important to point out that the ability to find any value within the hash table in a single step only works if we know the value’s key. If we tried to find a particular value without knowing its key, we’d still have to resort to searching each and every key-value pair within the hash table, which is O(N).``` ([Location 2835](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2835))
	- ```Similarly, we can only do O(1) lookups when using a key to find the value.``` ([Location 2837](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2837))
	- ```This is because the whole premise of the hash table is that the key determines the value’s location. But this premise only works in one direction: we use the key to find the value. The value does not determine the key’s location, so we have no way to easily find any key without combing through all of them.``` ([Location 2839](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2839))
	- ```some languages store the keys next to the values themselves. This is useful in case of collisions,``` ([Location 2843](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2843))
	- ```Each key can exist only once in the hash table, but there can be multiple instances of a value.``` ([Location 2844](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2844))
	- ```Trying to add data to a cell that is already filled is known as a collision.``` ([Location 2855](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2855))
	- ```One classic approach for handling collisions is known as separate chaining. When a collision occurs, instead of placing a single value in the cell, it places in it a reference to an array.``` ([Location 2855](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2855))
	- ```In a scenario where the computer hits upon a cell that references an array, its search can take some extra steps, as it needs to conduct a linear search within an array of multiple values.``` ([Location 2874](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2874))
	- ```If somehow all of our data ended up within a single cell of our hash table, our hash table would be no better than an array. So, it actually turns out that the worst-case performance for a hash table lookup is O(N).``` ([Location 2875](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2875))
	- ```it is critical that a hash table is designed in such a way that it will have few collisions, and therefore, typically perform lookups in O(1) time rather than O(N) time.``` ([Location 2877](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2877))
	- ```Ultimately, a hash table’s efficiency depends on three factors: How much data we’re storing in the hash table How many cells are available in the hash table Which hash function we’re using``` ([Location 2881](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2881))
	- ```If you have a lot of data and only a few cells, there will be many collisions and the hash table will lose its efficiency.``` ([Location 2883](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2883))
	- ```A good hash function, therefore, is one that distributes its data across all available cells.``` ([Location 2891](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2891))
	- ```The more we can spread out our data, the fewer collisions we will have.``` ([Location 2892](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2892))
	- ```In theory, then, the best way to avoid collisions would be to have a hash table with a large number of cells.``` ([Location 2893](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2893))
	- ```Although a hash table with 1,000 cells for our five pieces of data is great for avoiding collisions, we’d be using up 1,000 cells just to store just five pieces of data, and that’s a poor use of memory.``` ([Location 2896](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2896))
	- ```A good hash table strikes a balance of avoiding collisions while not consuming lots of memory.``` ([Location 2898](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2898))
	- ```To accomplish this, computer scientists have developed the following rule of thumb: for every 7 data elements stored in a hash table, it should have 10 cells.``` ([Location 2899](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2899))
	- ```This ratio of data to cells is called the load factor.``` ([Location 2901](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2901))
	- ```If you initially stored 7 pieces of data in a hash table, the computer might allocate a hash table with 10 cells. When you begin to add more data, though, the computer will expand the hash table by adding more cells and changing the hash``` ([Location 2902](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2902))
	- ```function so that the new data will be distributed evenly across the new cells.``` ([Location 2903](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2903))
	- ```In fact, in Python, hash tables are known as dictionaries since a dictionary is a common form of paired data: it’s a list of words with their respective definitions.``` ([Location 2912](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=2912))
	- ```If our lookup returns any value, it means the key itself must be in the hash table. If we get back nil, then the key must not be in the hash table.``` ([Location 3000](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3000))
	- ```When we analyze the efficiency of this algorithm, we find that it’s O(N * M),``` ([Location 3085](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3085))
	- ```If we say that N is the total number of items of both arrays combined, our algorithm is O(N),``` ([Location 3172](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3172))
	- ```That’s a huge win over our first algorithm, which was O(N * M).``` ([Location 3174](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3174))
	- ```In this chapter, you’re going to discover two new data structures: stacks and queues.``` ([Location 3204](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3204))
	- ```They’re simply arrays with restrictions. Yet, these restrictions are exactly what make them so elegant.``` ([Location 3205](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3205))
	- ```More specifically, stacks and queues are elegant tools for handling temporary data.``` ([Location 3205](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3205))
	- ```Temporary data is information that doesn’t have any meaning after it’s processed, so you can throw it away once you’re done with it.``` ([Location 3209](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3209))
	- ```Stacks and queues handle this kind of temporary data, but have a special focus on the order in which the data is handled,``` ([Location 3210](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3210))
	- ```stacks have the following three constraints: Data can be inserted only at the end of a stack. Data can be deleted only from the end of a stack. Only the last element of a stack can be read.``` ([Location 3212](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3212))
	- ```most computer science literature refers to the end of the stack as its top, and the beginning of the stack as its bottom.``` ([Location 3216](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3216))
	- ```Inserting a new value into a stack is also called pushing onto the stack.``` ([Location 3220](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3220))
	- ```Removing elements from the top of the stack is called popping from the stack.``` ([Location 3226](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3226))
	- ```A handy acronym used to describe stack operations is LIFO, which stands for “Last In, First Out.” All this means is that the last item pushed onto a stack is always the first item popped from it.``` ([Location 3231](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3231))
	- ```Most programming languages don’t actually come with the stack as a built-in data type or class. Instead, it’s up to you to implement it yourself.``` ([Location 3234](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3234))
	- ```Whenever a stack is initiated, we automatically build an empty array with @data = [].``` ([Location 3264](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3264))
	- ```Our stack also contains methods that push new elements to the @data array, pop elements from @data array, and read elements from the @data array.``` ([Location 3265](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3265))
	- ```However, by building the Stack class around the array, we have built an interface that forces the user to interact with the array in limited ways.``` ([Location 3268](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3268))
	- ```The stack data structure, then, isn’t the same kind of data structure that an array is.``` ([Location 3270](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3270))
	- ```The stack, on the other hand, is really a set of rules and processes around how we should interact with an array so that we can achieve a particular result.``` ([Location 3272](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3272))
	- ```the stack is an example of what is known as an abstract data type—it’s a kind of data structure that is a set of theoretical rules that revolve around some other built-in data structure.``` ([Location 3275](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3275))
	- ```The set that we encountered in Chapter 1, ​Why Data Structures Matter​ is another example of an abstract data type.``` ([Location 3276](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3276))
	- ```Some implementations of sets use arrays under the hood, while other implementations actually use hash tables.``` ([Location 3277](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3277))
	- ```It should be noted that even a built-in data structure can be an abstract data type.``` ([Location 3280](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3280))
	- ```down. By using a stack, we’re forced into only removing items from the top, as it’s impossible to get the stack to remove any other item.``` ([Location 3488](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3488))
	- ```As soon as someone sees a stack being used within an algorithm, they immediately know that the algorithm is working with a LIFO-based process.``` ([Location 3492](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3492))
	- ```A queue is another data structure designed to process temporary data.``` ([Location 3497](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3497))
	- ```Like a stack, a queue is also an abstract data type.``` ([Location 3498](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3498))
	- ```Like stacks, queues are arrays with three restrictions (it’s just a different set of restrictions): Data can be inserted only at the end of a queue. (This is identical behavior to the stack.) Data can be deleted only from the front of a queue. (This is the opposite behavior of the stack.)``` ([Location 3503](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3503))
	- ```Only the element at the front of a queue can be read. (This, too, is the opposite of behavior of the stack.)``` ([Location 3505](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3505))
	- ```a common term for inserting into a queue is enqueue,``` ([Location 3507](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3507))
	- ```As of now, the queue has functioned just like a stack. However, removing data happens in the reverse,``` ([Location 3510](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3510))
	- ```Removing an element from a queue is also known as dequeuing.``` ([Location 3511](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3511))
	- ```our Queue class wraps the array with an interface that restricts our interaction with the data, only allowing us to process the data in specific ways. The enqueue method allows us to insert data at the end of the array, while the dequeue removes the first item from the array. And the read method allows us to peek at just the very first element of the array.``` ([Location 3544](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3544))
	- ```Queues are also the perfect tool for handling asynchronous requests—they ensure that the requests are processed in the order in which they were received.``` ([Location 3627](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3627))
	- ```When used correctly, recursion can be used to solve certain types of tricky problems in surprisingly simple ways.``` ([Location 3644](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3644))
	- ```Recursion is the term for a function calling itself.``` ([Location 3652](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3652))
	- ```In almost any case in which you can use a loop, you can also use recursion.``` ([Location 3686](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3686))
	- ```just because you can use recursion doesn’t mean that you should use recursion.``` ([Location 3686](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3686))
	- ```In recursion terminology, the case in which our function will not recurse is known as the base case.``` ([Location 3711](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3711))
	- ```every recursive function needs at least one base case to prevent it from calling itself indefinitely.``` ([Location 3713](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3713))
	- ```To walk through the code to see what it does, here’s the process I recommend: Identify the base case. Walk through the function for the base case. Identify the “next-to-last” case. This is the case just before the base case, as I’ll demonstrate momentarily. Walk through the function for the “next-to-last” case. Repeat this process by identifying the case before the one you just analyzed, and walking though the function for that case.``` ([Location 3731](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3731))
	- ```it must be that the following code refers to the base case, since this is the case in which the function does not call itself:``` ([Location 3750](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3750))
	- ```If we call factorial(1), the method simply returns 1.``` ([Location 3761](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3761))
	- ```Until the computer reaches the end keyword of factorial(3), it’s not done with factorial(3). So, we enter into a weird situation. The computer did not yet complete executing factorial(3), yet it’s starting to run factorial(2) while still in the middle of factorial(3).``` ([Location 3795](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3795))
	- ```And factorial(2) isn’t the end of the story, because factorial(2) triggers factorial(1).``` ([Location 3799](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3799))
	- ```while still in the middle of running factorial(3), the computer calls factorial(2). And while running factorial(2), the computer runs factorial(1). It turns out then, that factorial(1) runs in the middle of both factorial(2) and factorial(3).``` ([Location 3801](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3801))
	- ```The computer uses a stack to keep track of which functions it’s in the middle of calling. This stack is known, appropriately enough, as the call stack.``` ([Location 3809](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3809))
	- ```Here’s how the call stack works in the context of our factorial example.``` ([Location 3811](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3811))
	- ```In order to track that the computer is still in the middle of factorial(3), the computer pushes that information onto a call stack:``` ([Location 3813](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3813))
	- ```Really, the computer also needs to save which line it’s in the middle of, and some other things like variable values, but I’m keeping the diagrams simple.)``` ([Location 3816](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3816))
	- ```After the computer completes factorial(1), it checks the call stack to see whether it’s in the middle of any other functions.``` ([Location 3824](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3824))
	- ```This is ideal for recursion, since the top element will be the most recently``` ([Location 3826](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3826))
	- ```called function, which is what the computer needs to wrap up next.``` ([Location 3827](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3827))
	- ```the function that was called last (that is, most recently), is the function we need to complete first.``` ([Location 3828](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3828))
	- ```At this point, the stack is empty, so the computer knows it’s done executing all of the methods, and the recursion is complete.``` ([Location 3833](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3833))
	- ```Some refer to this idea as passing a value up through the call stack.``` ([Location 3845](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3845))
	- ```That is, each recursive function returns its computed value to its “parent” function.``` ([Location 3846](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3846))
	- ```In the case of infinite recursion, the computer keeps pushing the same function again and again onto the call stack.``` ([Location 3849](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3849))
	- ```The call stack grows and grows until, eventually, the computer reaches a point where there’s simply no more room in its short-term memory to hold all this data.``` ([Location 3850](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3850))
	- ```This causes an error known as stack overflow``` ([Location 3851](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3851))
	- ```One type of problem in which recursion is a natural fit is when we need to delve into multiple layers of a problem without knowing how many layers there are.``` ([Location 3854](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3854))
	- ```As you’ve seen with the filesystem example, recursion is often a great choice for an algorithm in which the algorithm needs to dig into an arbitrary number of levels deep into something.``` ([Location 3957](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=3957))
	- ```Generally speaking, there are two basic approaches to manipulating data.``` ([Location 4083](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4083))
	- ```The first option is to create a new array that contains the “doubled” data,``` ([Location 4085](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4085))
	- ```The second option is called in-place modification, meaning the function actually changes the original array passed into function.``` ([Location 4095](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4095))
	- ```A second area in which recursion shines is where it is able to make a calculation based on a subproblem of the problem at hand.``` ([Location 4187](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4187))
	- ```and that is to calculate the factorial based on its subproblem.``` ([Location 4206](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4206))
	- ```A subproblem is a version of the very same problem applied to a smaller input.``` ([Location 4206](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4206))
	- ```In this implementation, we have three parameters. n, as before, is the number whose factorial we’re computing. i is a simple variable that starts at 1 and increments by one in each successive call until it reaches n. Finally, product is the variable in which we store the calculation as we keep multiplying each successive number.``` ([Location 4243](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4243))
	- ```While we can use recursion in this way to achieve the bottom-up approach, it’s not particularly elegant and does not add much value over using a classic loop.``` ([Location 4247](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4247))
	- ```But to go top down, we need recursion.``` ([Location 4249](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4249))
	- ```recursion shines when implementing a top-down approach because going top down offers a new mental strategy for tackling a problem.``` ([Location 4252](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4252))
	- ```I found that when tackling a top-down problem, it helps to think the following three thoughts: Imagine the function you’re writing has already been implemented by someone else. Identify the subproblem of the problem. See what happens when you call the function on the subproblem and go from there.``` ([Location 4270](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4270))
	- ```Let’s say we have a staircase of N steps, and a person has the ability to climb one, two, or three steps at a time. How many different possible “paths” can someone take to reach the top? Write a function that will calculate this for N steps.``` ([Location 4430](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4430))
	- ```Obviously, if there’s only one step, there’s only one possible path.``` ([Location 4435](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4435))
	- ```The person can climb one step twice, or the person can jump up two steps at once.``` ([Location 4435](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4435))
	- ```If we knew how many possible paths there are to climb a 10-step staircase, can we use that as a base for calculating the paths for an 11-step staircase?``` ([Location 4457](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4457))
	- ```we do know that climbing an 11-step staircase will take at least as many steps as climbing a 10-step staircase.``` ([Location 4458](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4458))
	- ```we’ll realize that if you’re taking any path that includes going from stair 10 to stair 11, you’re not taking any of the paths that include jumping from stair 9 to stair 11.``` ([Location 4461](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4461))
	- ```We’ve determined, then, that the number of steps to the top is at least the sum of all the paths to stairs 10, 9, and 8.``` ([Location 4465](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4465))
	- ```However, in thinking about it even further, it’s evident there aren’t any other possible paths to the top beyond these. After all, one cannot jump from stair 7 to stair 11. So, we can conclude that for N steps, the number of paths is:``` ([Location 4466](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4466))
	- ```We’re going to write a function that returns an array of all anagrams of a given string. An anagram is a reordering of all the characters within a string. For example, the anagrams of "abc" are:``` ([Location 4533](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4533))
	- ```For a string containing three characters, we create permutations that start with each of the three characters. Each permutation then picks its middle character from one of the two remaining characters, and its last character from the last character that’s left. This is 3 * 2 * 1, which is six permutations.``` ([Location 4634](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4634))
	- ```For a string of length N, we produce N! anagrams. In Big O Notation then, this is expressed as O(N!). This is also known as factorial time.``` ([Location 4645](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4645))
	- ```Although O(N!) is extremely slow, we don’t have a better option here, since our task is to generate all the anagrams, and there simply are N! anagrams for an N-character word.``` ([Location 4648](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4648))
	- ```However, while recursion can certainly solve some problems, it can also create new ones if not used properly.``` ([Location 4676](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4676))
	- ```recursion is often the culprit behind some of the slowest categories of Big O, such as O(2N).``` ([Location 4677](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4677))
	- ```improved version of the max function, the function recursively calls itself as many times as there are values in the array. We’d call this O(N).``` ([Location 4814](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4814))
	- ```the cases of O(N) we’ve seen involved loops, with a loop running N times.``` ([Location 4815](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4815))
	- ```However, we can apply the same principles of Big O to recursion as well.``` ([Location 4816](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4816))
	- ```Big O answers the key question: if there are N data elements, how many steps will the algorithm take?``` ([Location 4816](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4816))
	- ```When we increase the data by one, we roughly double the number of steps the algorithm takes.``` ([Location 4824](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4824))
	- ```As you learned by our discussion of the ​Password Cracker​, this is the pattern of O(2N).``` ([Location 4825](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4825))
	- ```This is a powerful lesson: avoiding extra recursive calls is key to keeping recursion fast. What at first glance was a very small change to our code—the mere storing of a computation in a variable—ended up changing the speed of our function from O(2N) to O(N).``` ([Location 4829](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4829))
	- ```which sums the previous two numbers in the Fibonacci series. It’s a beautiful recursive function.``` ([Location 4850](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4850))
	- ```alarm bells should be going off in your head right now because our function calls itself twice.``` ([Location 4851](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4851))
	- ```As we’ve seen, a function calling itself twice can easily lead us down the road to O(2N).``` ([Location 4855](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4855))
	- ```This is a case of what computer scientists call overlapping subproblems.``` ([Location 4861](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4861))
	- ```When a problem is solved by solving smaller versions of the same problem, the smaller problem is called a subproblem.``` ([Location 4862](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4862))
	- ```What makes these subproblems overlapping, though, is the fact that fib(n - 2) and fib(n - 1) end up calling many of the same functions as each other.``` ([Location 4865](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4865))
	- ```Dynamic programming is the process of optimizing recursive problems that have overlapping subproblems.``` ([Location 4873](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4873))
	- ```Optimizing an algorithm with dynamic programming is typically accomplished with one of two techniques. The first technique is something called memoization. And no, that’s not a typo. Pronounced meh-moe-ih-ZAY-shun, memoization is a simple, but brilliant technique for reducing recursive calls in cases of overlapping subproblems.``` ([Location 4876](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4876))
	- ```memoization reduces recursive calls by remembering previously computed functions.``` ([Location 4878](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4878))
	- ```our code will memoize the results of all new computations it encounters. After encountering fib(4), fib(5), and fib(6), for example, our hash table will look like this: ​  { ​  3: 2, ​  4: 3, ​  5: 5, ​  6: 8 ​  }``` ([Location 4884](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4884))
	- ```Without memoization, fib(4) would normally call fib(3) and fib(2), which in turn make their own recursive calls. Now that we have this hash table, we can approach things differently. Instead of fib(4) just blithely calling fib(3), for example, it first checks the hash table to see if the result of fib(3) has already been computed. Only if the 3 key is not in the hash table does the function proceed to call fib(3).``` ([Location 4894](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4894))
	- ```Memoization goes for the jugular of overlapping subproblems. The whole issue with overlapping subproblems is that we end up computing the same recursive calls over and over again. With memoization, though, each time we make a new calculation, we store it in the hash table for future use. This way, we only make a calculation if it hadn’t ever been made before.``` ([Location 4900](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4900))
	- ```How does each recursive function get access to this hash table? The answer is: we pass the hash table as a second parameter to the function.``` ([Location 4903](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4903))
	- ```Because the hash table is a specific object in memory, we’re able to pass it from one recursive call to the next, even though we’re modifying it``` ([Location 4904](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4904))
	- ```The second technique, known as going bottom-up, is a lot less fancy and may not even seem like a technique at all. All going bottom-up means is to ditch recursion and use some other approach (like a loop) to solve the same problem.``` ([Location 4975](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4975))
	- ```The reason that going bottom-up is considered part of dynamic programming is because dynamic programming means taking a problem that could be solved recursively and ensure that it doesn’t make duplicate calls for overlapping subproblems.``` ([Location 4977](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4977))
	- ```Using iteration (that is, loops) instead of recursion is, technically, a way to achieve this.``` ([Location 4979](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4979))
	- ```Going bottom-up becomes more of a “technique” when the problem is more naturally solved with recursion.``` ([Location 4980](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=4980))
	- ```Usually, it depends on the problem and why you’re using recursion in the first place. If recursion presents an elegant and intuitive solution to a given problem, you may want to stick with it and use memoization to deal with any overlapping subproblems.``` ([Location 5028](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5028))
	- ```if the iterative approach is equally intuitive, you may want to go with that.``` ([Location 5030](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5030))
	- ```It’s important to point out that even with memoization, recursion does carry some extra overhead versus iteration.``` ([Location 5030](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5030))
	- ```with any recursion, the computer needs to keep track of all the calls in a call stack, which consumes memory. The memoization itself also requires the use of a hash table, which will take up additional space on``` ([Location 5031](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5031))
	- ```your computer as well.``` ([Location 5033](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5033))
	- ```Generally speaking, going bottom-up is often the better choice unless the recursive solution is more intuitive.``` ([Location 5034](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5034))
	- ```in many of these languages, the sorting algorithm that is employed under the hood is Quicksort.``` ([Location 5083](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5083))
	- ```Quicksort relies on a concept called partitioning,``` ([Location 5088](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5088))
	- ```To partition an array is to take a random value from the array—which is then called the pivot—and make sure that every number that is less than the pivot ends up to the left of the pivot, and that every number greater than the pivot ends up to the right of the pivot.``` ([Location 5089](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5089))
	- ```For consistency’s sake, we’ll always select the rightmost value to be our pivot``` ([Location 5093](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5093))
	- ```We then assign “pointers”—one to the left-most value of the array, and one to the rightmost value of the array, excluding the pivot itself:``` ([Location 5095](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5095))
	- ```The right pointer will also stop if it reaches the beginning of the array.``` ([Location 5100](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5100))
	- ```If the left pointer has reached (or gone beyond) the right pointer, we move on to Step 4. Otherwise, we swap the values that the left and right pointers are pointing to, and then go back to repeat Steps 1, 2, and 3 again.``` ([Location 5101](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5101))
	- ```When we’re done with a partition, we are now assured that all values to the left of the pivot are less than the pivot, and all values to the right of the pivot are greater than it.``` ([Location 5104](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5104))
	- ```The Quicksort algorithm is a combination of partitions and recursion. It works as follows: Partition the array. The pivot is now in its proper place. Treat the subarrays to the left and right of the pivot as their own arrays, and recursively repeat Steps 1 and 2. That means we’ll partition each subarray and end up with even smaller sub-subarrays to the left and right of each subarray’s pivot. We then partition those sub-subarrays, and so on and so forth. When we have a subarray that has zero or one elements, that is our base case and we do nothing.``` ([Location 5266](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5266))
	- ```At this point, the left pointer is pointing to a value that is equal to the pivot (since it is the pivot!), and so the left pointer stops. Note how the left pointer managed to sneak past the right pointer. That’s okay, though. The algorithm is designed to work even with such an occurrence.``` ([Location 5286](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5286))
	- ```Now, we activate the right pointer. However, because the right pointer’s value (1) is less than the pivot, it stays still.``` ([Location 5288](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5288))
	- ```Next, we swap the pivot with the left pointer’s value. Now, it just so happens that the left pointer is pointing to the pivot itself, so we swap the pivot with itself,``` ([Location 5290](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5290))
	- ```We compare the left pointer (6) with the pivot (5). Since 6 is greater than the pivot, the left pointer doesn’t move further.``` ([Location 5320](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5320))
	- ```When we break down the steps of a partition, we’ll note that a partition involves two primary types of steps: Comparisons: We compare each of the values at hand to the pivot. Swaps: When appropriate, we swap the values being pointed to by the left and right pointers.``` ([Location 5386](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5386))
	- ```For randomly sorted data,``` ([Location 5393](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5393))
	- ```we generally swap about half of the values. On average, then, we’re making about N / 4 swaps.``` ([Location 5394](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5394))
	- ```We’ve left out the actual numbers from the array since the exact values don’t matter. Note that in the diagram, the active subarray is the group of cells that is not grayed out.``` ([Location 5400](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5400))
	- ```Since Quicksort is essentially comprised of this series of partitions, and each partition takes about N steps for N elements of each subarray, if we add the sizes of all the subarrays together, we’ll get the total number of steps Quicksort takes:``` ([Location 5404](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5404))
	- ```This assumes a best- or average-case scenario, where the pivot ends up roughly in the middle of the subarray after each partition.``` ([Location 5426](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5426))
	- ```How many times can we break an array into halves until we’ve broken it completely down to the point of where each subarray is of size 1? For an array of size N, this will take us log N times. Take a look at the following diagram:``` ([Location 5445](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5445))
	- ```this is why Quicksort takes N * log N steps. We have log N halvings, and for each halving, we perform a partition on all the subarrays whose elements add up to N.``` ([Location 5449](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5449))
	- ```For many other algorithms we’ve encountered, the best case was one where the array was already sorted. When it comes to Quicksort, however, the best-case scenario is one in which the pivot always ends up smack in the middle of the subarray after the partition.``` ([Location 5457](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5457))
	- ```The worst-case scenario for Quicksort is one in which the pivot always ends up on one side of the subarray instead of in the middle.``` ([Location 5460](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5460))
	- ```So, in this worst-case scenario, we have partitions of 8 + 7 + 6 + 5 + 4 + 3 + 2 + 1 elements, which yields a total of 36 comparisons.``` ([Location 5466](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5466))
	- ```To put this a little more formulaically, we’d say that for N elements, there are N + (N - 1) + (N - 2) + (N - 3) … + 1 steps.``` ([Location 5467](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5467))
	- ```So, in a worst-case scenario, Quicksort has an efficiency of O(N2).``` ([Location 5470](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5470))
	- ```We can see they have identical worst-case scenarios, and that Insertion Sort is actually faster than Quicksort in a best-case scenario. However, the reason Quicksort is superior to Insertion Sort is because of the average scenario—which, again, is what happens most of the time. For average cases, Insertion Sort takes a whopping O(N2), while Quicksort is much faster at O(N log N).``` ([Location 5475](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5475))
	- ```However, there is a very similar algorithm that can come in handy for practical cases—and it’s called Quickselect.``` ([Location 5479](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5479))
	- ```but you do want to know the tenth-lowest value in the array, or the fifth-highest.``` ([Location 5481](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5481))
	- ```This can be useful if we had a lot of test grades and want to know what the 25th percentile was,``` ([Location 5482](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5482))
	- ```or if we want to find the median grade.``` ([Location 5483](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5483))
	- ```even were we to use a fast sorting algorithm like Quicksort, this algorithm would take at least O(N log N) for average cases.``` ([Location 5484](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5484))
	- ```we can do even better with a brilliant little algorithm known as Quickselect.``` ([Location 5485](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5485))
	- ```Quickselect relies on partitioning, and can be thought of as a hybrid of Quicksort and binary search.``` ([Location 5485](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5485))
	- ```One of the beautiful things about Quickselect is that we can find the correct value without having to sort the entire array.``` ([Location 5502](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5502))
	- ```Mergesort is another well-known O(N log N) sorting algorithm, and I recommend you look it up, as it is a beautiful recursive algorithm.``` ([Location 5569](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5569))
	- ```We’ve used sorting to develop an algorithm that is O(N log N), which is a significant improvement over the original O(N2) algorithm.``` ([Location 5626](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5626))
	- ```Plenty of algorithms employ sorting as part of a larger process. We now know that any time we do so, we have an algorithm that is at least O(N log N). Of course, the algorithm may be slower than this if it has other things going on, but we know that O(N log N) will always be the baseline.``` ([Location 5627](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5627))
	- ```The Quicksort and Quickselect algorithms are recursive algorithms that present beautiful and efficient solutions to thorny problems. They’re great examples of how a non-obvious but well-thought-out algorithm can boost performance.``` ([Location 5629](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5629))
	- ```nodes are pieces of data that may be dispersed throughout the computer’s memory. Node-based data structures offer new ways to organize and access data that provide a number of major performance advantages.``` ([Location 5665](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5665))
	- ```the linked list, which is the simplest node-based data structure``` ([Location 5666](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5666))
	- ```linked lists seem almost identical to arrays, but come with their own set of trade-offs in efficiency that can give us a performance boost for certain situations.``` ([Location 5667](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5667))
	- ```Like an array, a linked list is a data structure that represents a list of items.``` ([Location 5669](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5669))
	- ```You also saw that the computer has the ability to access any memory address in one step, and can use that power to also immediately access any index within the array.``` ([Location 5674](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5674))
	- ```Linked lists, on the other hand, work quite differently. Instead of being a contiguous block of memory, the data from linked lists can be scattered across different cells throughout the computer’s memory.``` ([Location 5677](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5677))
	- ```Connected data that is dispersed throughout memory are known as nodes.``` ([Location 5679](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5679))
	- ```if the nodes are not next to each other in memory, how does the computer know which nodes are part of the same linked list? This is the key to the linked list: each node also comes with a little extra information, namely, the memory address of the next node in the list.``` ([Location 5680](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5680))
	- ```This extra piece of data—this pointer to the next node’s memory address—is known as a link.``` ([Location 5682](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5682))
	- ```The first cell holds the actual data, while the second cell serves as a link that indicates where in memory the next node begins. The final node’s link contains null since the linked list ends there.``` ([Location 5687](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5687))
	- ```A linked list’s first node can also be referred to as its head, and its final node as its tail.)``` ([Location 5689](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5689))
	- ```The fact that a linked list’s data can be spread throughout the computer’s memory is a potential advantage it has over the array.``` ([Location 5691](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5691))
	- ```A really important point emerges: when dealing with a linked list, we only have immediate access to its first node.``` ([Location 5764](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5764))
	- ```All our program knows immediately is the memory address of the first node of the linked list. However, it doesn’t know offhand where any of the other nodes are.``` ([Location 5771](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5771))
	- ```it accesses the first node. It then follows the first node’s link to the second node, and then the second node’s link to the third node.``` ([Location 5773](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5773))
	- ```It turns out, then, that if we were to read from the last node in the list, it would take N steps for N nodes in the list.``` ([Location 5775](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5775))
	- ```Linked lists having a worst-case read of O(N) is a major disadvantage when compared with arrays that can read any element in just O(1).``` ([Location 5776](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=5776))
	- ```One case where linked lists shine is when we examine a single list and delete many elements from it.``` ([Location 6110](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6110))
	- ```A doubly linked list is like a linked list except that each node has two links—one that points to the next node, and another that points to the previous node. In addition, the doubly linked list always keeps track of both the first and last nodes, instead of just the first node.``` ([Location 6124](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6124))
	- ```Because doubly linked lists can insert data at the end in O(1) time and delete data from the front in O(1) time, they make the perfect underlying data structure for a queue.``` ([Location 6207](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6207))
	- ```You learned there that queues are an example of an abstract data type, and that we were able to use an array to implement them under the hood.``` ([Location 6210](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6210))
	- ```what do we do if we want a data structure that maintains order yet also has fast search, insertion, and deletion?``` ([Location 6367](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6367))
	- ```Enter the binary search tree.``` ([Location 6369](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6369))
	- ```We’d say, then, that searching in a binary search tree is O(log N),``` ([Location 6438](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6438))
	- ```if there are N nodes in a balanced binary tree, there will be about log N levels (that is, rows).``` ([Location 6441](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6441))
	- ```while search in a binary search tree is O(log N), so is binary search within an ordered array, in which each number we select also eliminates half of the remaining possible values. In this regard, then, searching a binary search tree has the same efficiency as binary search within an ordered array.``` ([Location 6455](https://readwise.io/to_kindle?action=open&asin=B08KYMK4NR&location=6455))